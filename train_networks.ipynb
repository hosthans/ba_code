{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 15:40:04.075750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from training.utils.trainer import Trainer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train mnist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'mnist', 'model_name': 'VGG16', 'gray': True, 'torch': True, 'num_classes': 10, 'local': False}\n",
      "----------------Loading datasets-----------------\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets/mnist\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=3)\n",
      "               ToTensor()\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n",
      "Draw Graph in Tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1065: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Process started!\n",
      "Epoch:0\tTime:67.96\tTrain Loss:0.08\tTrain Acc:97.74\tTest Acc99.26\n",
      "Epoch:1\tTime:63.98\tTrain Loss:0.02\tTrain Acc:99.51\tTest Acc99.50\n",
      "Epoch:2\tTime:64.00\tTrain Loss:0.01\tTrain Acc:99.80\tTest Acc99.50\n",
      "Epoch:3\tTime:64.04\tTrain Loss:0.00\tTrain Acc:99.89\tTest Acc99.53\n",
      "Epoch:4\tTime:63.79\tTrain Loss:0.00\tTrain Acc:99.93\tTest Acc99.58\n",
      "Epoch:5\tTime:64.02\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.58\n",
      "Epoch:6\tTime:63.78\tTrain Loss:0.00\tTrain Acc:99.97\tTest Acc99.65\n",
      "Epoch:7\tTime:63.77\tTrain Loss:0.00\tTrain Acc:99.97\tTest Acc99.62\n",
      "Epoch:8\tTime:63.72\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.59\n",
      "Epoch:9\tTime:64.34\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.63\n",
      "Epoch:10\tTime:64.21\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.60\n",
      "Epoch:11\tTime:63.81\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.59\n",
      "Epoch:12\tTime:63.69\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.62\n",
      "Epoch:13\tTime:64.34\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.62\n",
      "Epoch:14\tTime:64.09\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.63\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset=\"mnist\", mode=\"nn\")\n",
    "loss_nn, acc_nn, loss_t_nn, acc_t_nn = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/nn_mnist_training_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss_nn, \n",
    "        'loss_test': loss_t_nn,\n",
    "        'acc_train': acc_nn,\n",
    "        'acc_test': acc_t_nn\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train celebA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'celeba', 'indices': False, 'gray': False, 'train_file': 'datasets/meta_celeba/trainset.txt', 'test_file': 'datasets/meta_celeba/testset.txt', 'gan_file': 'datasets/meta_celeba/ganset.txt', 'img_path': 'datasets/celeba/img_align_celeba', 'img_gan_path': 'datasets/celeba/img_align_celeba', 'model_name': 'VGG16_celeba', 'num_classes': 1000}\n",
      "----------------Loading datasets-----------------\n",
      "Load 27018 images\n",
      "Load 3009 images\n",
      "<training.utils.dataloader.ImageFolder object at 0x7fb008053fd0>\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n",
      "Draw Graph in Tensorboard\n",
      "Training-Process started!\n",
      "Epoch:0\tTime:34.15\tTrain Loss:7.04\tTrain Acc:0.11\tTest Acc0.21\n",
      "Epoch:1\tTime:34.19\tTrain Loss:6.87\tTrain Acc:0.31\tTest Acc0.46\n",
      "Epoch:2\tTime:34.18\tTrain Loss:6.69\tTrain Acc:1.05\tTest Acc1.24\n",
      "Epoch:3\tTime:34.23\tTrain Loss:6.51\tTrain Acc:2.47\tTest Acc1.95\n",
      "Epoch:4\tTime:34.22\tTrain Loss:6.32\tTrain Acc:4.65\tTest Acc3.37\n",
      "Epoch:5\tTime:34.18\tTrain Loss:6.12\tTrain Acc:7.39\tTest Acc4.79\n",
      "Epoch:6\tTime:34.19\tTrain Loss:5.92\tTrain Acc:10.26\tTest Acc6.46\n",
      "Epoch:7\tTime:34.15\tTrain Loss:5.72\tTrain Acc:13.13\tTest Acc7.42\n",
      "Epoch:8\tTime:34.17\tTrain Loss:5.53\tTrain Acc:16.36\tTest Acc10.01\n",
      "Epoch:9\tTime:34.36\tTrain Loss:5.34\tTrain Acc:19.26\tTest Acc11.26\n",
      "Epoch:10\tTime:34.46\tTrain Loss:5.15\tTrain Acc:22.54\tTest Acc13.35\n",
      "Epoch:11\tTime:34.39\tTrain Loss:4.97\tTrain Acc:25.17\tTest Acc14.91\n",
      "Epoch:12\tTime:34.52\tTrain Loss:4.80\tTrain Acc:28.31\tTest Acc16.94\n",
      "Epoch:13\tTime:34.46\tTrain Loss:4.62\tTrain Acc:31.30\tTest Acc18.71\n",
      "Epoch:14\tTime:34.53\tTrain Loss:4.46\tTrain Acc:33.90\tTest Acc20.81\n",
      "Epoch:15\tTime:34.64\tTrain Loss:4.30\tTrain Acc:36.69\tTest Acc22.51\n",
      "Epoch:16\tTime:34.57\tTrain Loss:4.15\tTrain Acc:39.47\tTest Acc24.04\n",
      "Epoch:17\tTime:34.52\tTrain Loss:4.00\tTrain Acc:42.24\tTest Acc25.53\n",
      "Epoch:18\tTime:34.18\tTrain Loss:3.85\tTrain Acc:44.89\tTest Acc27.81\n",
      "Epoch:19\tTime:34.12\tTrain Loss:3.71\tTrain Acc:47.22\tTest Acc28.84\n",
      "Epoch:20\tTime:34.29\tTrain Loss:3.57\tTrain Acc:49.90\tTest Acc30.97\n",
      "Epoch:21\tTime:34.13\tTrain Loss:3.44\tTrain Acc:52.16\tTest Acc32.14\n",
      "Epoch:22\tTime:34.22\tTrain Loss:3.31\tTrain Acc:54.74\tTest Acc33.52\n",
      "Epoch:23\tTime:34.26\tTrain Loss:3.19\tTrain Acc:56.77\tTest Acc35.40\n",
      "Epoch:24\tTime:34.27\tTrain Loss:3.07\tTrain Acc:59.07\tTest Acc36.54\n",
      "Epoch:25\tTime:34.45\tTrain Loss:2.95\tTrain Acc:61.18\tTest Acc38.07\n",
      "Epoch:26\tTime:34.43\tTrain Loss:2.83\tTrain Acc:62.97\tTest Acc38.99\n",
      "Epoch:27\tTime:34.23\tTrain Loss:2.72\tTrain Acc:65.00\tTest Acc41.23\n",
      "Epoch:28\tTime:34.42\tTrain Loss:2.61\tTrain Acc:66.72\tTest Acc42.40\n",
      "Epoch:29\tTime:34.08\tTrain Loss:2.51\tTrain Acc:68.55\tTest Acc43.50\n",
      "Epoch:30\tTime:34.40\tTrain Loss:2.41\tTrain Acc:70.27\tTest Acc45.03\n",
      "Epoch:31\tTime:34.43\tTrain Loss:2.31\tTrain Acc:71.93\tTest Acc46.48\n",
      "Epoch:32\tTime:34.42\tTrain Loss:2.22\tTrain Acc:73.40\tTest Acc47.44\n",
      "Epoch:33\tTime:34.31\tTrain Loss:2.13\tTrain Acc:75.21\tTest Acc48.22\n",
      "Epoch:34\tTime:34.33\tTrain Loss:2.04\tTrain Acc:76.17\tTest Acc48.69\n",
      "Epoch:35\tTime:34.40\tTrain Loss:1.96\tTrain Acc:77.38\tTest Acc50.18\n",
      "Epoch:36\tTime:34.39\tTrain Loss:1.87\tTrain Acc:78.75\tTest Acc51.10\n",
      "Epoch:37\tTime:34.33\tTrain Loss:1.79\tTrain Acc:80.26\tTest Acc51.63\n",
      "Epoch:38\tTime:34.60\tTrain Loss:1.72\tTrain Acc:81.33\tTest Acc52.91\n",
      "Epoch:39\tTime:34.50\tTrain Loss:1.65\tTrain Acc:82.34\tTest Acc53.16\n",
      "Epoch:40\tTime:34.66\tTrain Loss:1.58\tTrain Acc:83.31\tTest Acc53.76\n",
      "Epoch:41\tTime:34.50\tTrain Loss:1.51\tTrain Acc:84.63\tTest Acc55.04\n",
      "Epoch:42\tTime:34.39\tTrain Loss:1.45\tTrain Acc:85.41\tTest Acc55.18\n",
      "Epoch:43\tTime:34.62\tTrain Loss:1.39\tTrain Acc:86.27\tTest Acc55.82\n",
      "Epoch:44\tTime:34.86\tTrain Loss:1.33\tTrain Acc:87.38\tTest Acc55.86\n",
      "Epoch:45\tTime:34.36\tTrain Loss:1.27\tTrain Acc:87.85\tTest Acc56.36\n",
      "Epoch:46\tTime:34.44\tTrain Loss:1.22\tTrain Acc:88.62\tTest Acc56.50\n",
      "Epoch:47\tTime:34.32\tTrain Loss:1.17\tTrain Acc:89.46\tTest Acc57.46\n",
      "Epoch:48\tTime:34.24\tTrain Loss:1.12\tTrain Acc:90.02\tTest Acc57.85\n",
      "Epoch:49\tTime:34.70\tTrain Loss:1.07\tTrain Acc:90.76\tTest Acc58.63\n",
      "Epoch:50\tTime:34.67\tTrain Loss:1.02\tTrain Acc:91.50\tTest Acc58.81\n",
      "Epoch:51\tTime:34.36\tTrain Loss:0.98\tTrain Acc:92.02\tTest Acc59.87\n",
      "Epoch:52\tTime:34.30\tTrain Loss:0.94\tTrain Acc:92.61\tTest Acc59.52\n",
      "Epoch:53\tTime:34.63\tTrain Loss:0.90\tTrain Acc:93.15\tTest Acc60.26\n",
      "Epoch:54\tTime:34.74\tTrain Loss:0.86\tTrain Acc:93.72\tTest Acc59.98\n",
      "Epoch:55\tTime:34.25\tTrain Loss:0.82\tTrain Acc:94.23\tTest Acc60.44\n",
      "Epoch:56\tTime:34.64\tTrain Loss:0.78\tTrain Acc:94.60\tTest Acc60.65\n",
      "Epoch:57\tTime:35.12\tTrain Loss:0.75\tTrain Acc:95.15\tTest Acc61.29\n",
      "Epoch:58\tTime:34.73\tTrain Loss:0.72\tTrain Acc:95.49\tTest Acc60.76\n",
      "Epoch:59\tTime:34.67\tTrain Loss:0.69\tTrain Acc:95.67\tTest Acc61.33\n",
      "Epoch:60\tTime:34.76\tTrain Loss:0.66\tTrain Acc:96.11\tTest Acc61.47\n",
      "Epoch:61\tTime:34.52\tTrain Loss:0.63\tTrain Acc:96.40\tTest Acc61.72\n",
      "Epoch:62\tTime:34.59\tTrain Loss:0.60\tTrain Acc:96.75\tTest Acc62.22\n",
      "Epoch:63\tTime:34.58\tTrain Loss:0.58\tTrain Acc:96.96\tTest Acc62.22\n",
      "Epoch:64\tTime:34.35\tTrain Loss:0.55\tTrain Acc:97.17\tTest Acc62.22\n",
      "Epoch:65\tTime:34.65\tTrain Loss:0.53\tTrain Acc:97.43\tTest Acc62.54\n",
      "Epoch:66\tTime:34.44\tTrain Loss:0.51\tTrain Acc:97.61\tTest Acc62.64\n",
      "Epoch:67\tTime:34.37\tTrain Loss:0.48\tTrain Acc:97.89\tTest Acc63.07\n",
      "Epoch:68\tTime:34.57\tTrain Loss:0.46\tTrain Acc:98.08\tTest Acc62.75\n",
      "Epoch:69\tTime:34.51\tTrain Loss:0.45\tTrain Acc:98.21\tTest Acc62.93\n",
      "Epoch:70\tTime:34.65\tTrain Loss:0.42\tTrain Acc:98.42\tTest Acc62.93\n",
      "Epoch:71\tTime:34.36\tTrain Loss:0.41\tTrain Acc:98.57\tTest Acc63.25\n",
      "Epoch:72\tTime:34.87\tTrain Loss:0.39\tTrain Acc:98.61\tTest Acc63.28\n",
      "Epoch:73\tTime:34.38\tTrain Loss:0.38\tTrain Acc:98.80\tTest Acc63.49\n",
      "Epoch:74\tTime:34.52\tTrain Loss:0.36\tTrain Acc:98.97\tTest Acc64.20\n",
      "Epoch:75\tTime:34.29\tTrain Loss:0.34\tTrain Acc:99.00\tTest Acc64.17\n",
      "Epoch:76\tTime:34.40\tTrain Loss:0.33\tTrain Acc:99.08\tTest Acc64.52\n",
      "Epoch:77\tTime:34.62\tTrain Loss:0.32\tTrain Acc:99.17\tTest Acc63.96\n",
      "Epoch:78\tTime:34.46\tTrain Loss:0.31\tTrain Acc:99.35\tTest Acc63.49\n",
      "Epoch:79\tTime:34.54\tTrain Loss:0.30\tTrain Acc:99.36\tTest Acc64.28\n",
      "Epoch:80\tTime:34.55\tTrain Loss:0.28\tTrain Acc:99.39\tTest Acc64.06\n",
      "Epoch:81\tTime:34.56\tTrain Loss:0.28\tTrain Acc:99.46\tTest Acc63.71\n",
      "Epoch:82\tTime:34.72\tTrain Loss:0.27\tTrain Acc:99.49\tTest Acc64.67\n",
      "Epoch:83\tTime:34.53\tTrain Loss:0.25\tTrain Acc:99.53\tTest Acc64.81\n",
      "Epoch:84\tTime:34.61\tTrain Loss:0.25\tTrain Acc:99.59\tTest Acc64.67\n",
      "Epoch:85\tTime:34.48\tTrain Loss:0.24\tTrain Acc:99.64\tTest Acc64.49\n",
      "Epoch:86\tTime:34.35\tTrain Loss:0.23\tTrain Acc:99.65\tTest Acc64.70\n",
      "Epoch:87\tTime:34.94\tTrain Loss:0.22\tTrain Acc:99.68\tTest Acc64.67\n",
      "Epoch:88\tTime:34.38\tTrain Loss:0.21\tTrain Acc:99.73\tTest Acc64.84\n",
      "Epoch:89\tTime:34.41\tTrain Loss:0.21\tTrain Acc:99.74\tTest Acc64.95\n",
      "Epoch:90\tTime:34.82\tTrain Loss:0.20\tTrain Acc:99.79\tTest Acc65.06\n",
      "Epoch:91\tTime:34.45\tTrain Loss:0.19\tTrain Acc:99.78\tTest Acc65.23\n",
      "Epoch:92\tTime:34.63\tTrain Loss:0.19\tTrain Acc:99.81\tTest Acc64.99\n",
      "Epoch:93\tTime:34.75\tTrain Loss:0.18\tTrain Acc:99.81\tTest Acc64.77\n",
      "Epoch:94\tTime:34.56\tTrain Loss:0.17\tTrain Acc:99.83\tTest Acc65.34\n",
      "Epoch:95\tTime:34.86\tTrain Loss:0.17\tTrain Acc:99.86\tTest Acc65.41\n",
      "Epoch:96\tTime:34.72\tTrain Loss:0.16\tTrain Acc:99.87\tTest Acc65.31\n",
      "Epoch:97\tTime:34.99\tTrain Loss:0.16\tTrain Acc:99.86\tTest Acc65.41\n",
      "Epoch:98\tTime:34.77\tTrain Loss:0.15\tTrain Acc:99.89\tTest Acc65.27\n",
      "Epoch:99\tTime:34.53\tTrain Loss:0.15\tTrain Acc:99.87\tTest Acc65.23\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset=\"celeba\", mode=\"nn\")\n",
    "loss_nn, acc_nn, loss_t_nn, acc_t_nn = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/nn_celeba_training_celeba_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss_nn, \n",
    "        'loss_test': loss_t_nn,\n",
    "        'acc_train': acc_nn,\n",
    "        'acc_test': acc_t_nn\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train dp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'mnist', 'model_name': 'VGG16', 'gray': True, 'torch': True, 'num_classes': 10, 'local': False}\n",
      "----------------Loading datasets-----------------\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n",
      "Draw Graph in Tensorboard\n",
      "[]\n",
      "Training-Process started! -> 15 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: overflow encountered in exp\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: overflow encountered in divide\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:139: RuntimeWarning: overflow encountered in exp\n",
      "  d2 = np.flip(np.flip(p * np.exp(-t)).cumsum())\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:139: RuntimeWarning: invalid value encountered in accumulate\n",
      "  d2 = np.flip(np.flip(p * np.exp(-t)).cumsum())\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  ndelta = np.exp(t) * d2 - d1\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:140: RuntimeWarning: invalid value encountered in multiply\n",
      "  ndelta = np.exp(t) * d2 - d1\n",
      "01/01/2024 15:57:47:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1324: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tTime:200.93\tTrain Loss:1.60\tTrain Acc:61.37\tTest Acc84.34\n",
      "Epoch:1\tTime:194.80\tTrain Loss:0.38\tTrain Acc:89.53\tTest Acc94.18\n",
      "Epoch:2\tTime:192.09\tTrain Loss:0.16\tTrain Acc:95.03\tTest Acc96.11\n",
      "Epoch:3\tTime:193.32\tTrain Loss:0.13\tTrain Acc:96.20\tTest Acc96.81\n",
      "Epoch:4\tTime:194.60\tTrain Loss:0.11\tTrain Acc:96.85\tTest Acc97.23\n",
      "Epoch:5\tTime:195.41\tTrain Loss:0.10\tTrain Acc:97.04\tTest Acc97.42\n",
      "Epoch:6\tTime:195.42\tTrain Loss:0.09\tTrain Acc:97.42\tTest Acc97.73\n",
      "Epoch:7\tTime:194.16\tTrain Loss:0.08\tTrain Acc:97.59\tTest Acc97.89\n",
      "Epoch:8\tTime:196.10\tTrain Loss:0.08\tTrain Acc:97.67\tTest Acc98.00\n",
      "Epoch:9\tTime:196.34\tTrain Loss:0.08\tTrain Acc:97.80\tTest Acc98.16\n",
      "Epoch:10\tTime:194.52\tTrain Loss:0.07\tTrain Acc:97.95\tTest Acc98.15\n",
      "Epoch:11\tTime:197.66\tTrain Loss:0.07\tTrain Acc:97.98\tTest Acc98.32\n",
      "Epoch:12\tTime:197.46\tTrain Loss:0.06\tTrain Acc:98.14\tTest Acc98.41\n",
      "Epoch:13\tTime:196.01\tTrain Loss:0.06\tTrain Acc:98.15\tTest Acc98.37\n",
      "Epoch:14\tTime:195.85\tTrain Loss:0.06\tTrain Acc:98.23\tTest Acc98.55\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(dataset=\"mnist\", mode=\"dpnn\")\n",
    "trainer.correct_module()\n",
    "loss, acc, loss_t, acc_t, eps = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/dp_training_mnist_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss, \n",
    "        'loss_test': loss_t,\n",
    "        'acc_train': acc,\n",
    "        'acc_test': acc_t,\n",
    "        'epsilons': eps\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
