{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:23:03.721928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from training.utils.trainer import Trainer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'celeba', 'indices': False, 'gray': False, 'train_file': 'datasets/meta_celeba/trainset.txt', 'test_file': 'datasets/meta_celeba/testset.txt', 'gan_file': 'datasets/meta_celeba/ganset.txt', 'img_path': 'datasets/celeba/img_align_celeba', 'img_gan_path': 'datasets/celeba/img_align_celeba', 'model_name': 'VGG16', 'num_classes': 1000}\n",
      "----------------Loading datasets-----------------\n",
      "Load 27018 images\n",
      "Load 3009 images\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw Graph in Tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1065: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Training-Process started! -> 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "12/08/2023 13:24:42:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1324: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tTime:157.11\tTrain Loss:7.03\tTrain Acc:0.11\tTest Acc0.10\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 7.78 GiB of which 90.25 MiB is free. Process 5120 has 1.95 GiB memory in use. Process 7102 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.59 GiB memory in use. Of the allocated memory 1.15 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(dataset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mceleba\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdpnn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39mcorrect_module()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss, acc, loss_t, acc_t, eps \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining_metrics/dp_training_data_celeba.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_to_save \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_train\u001b[39m\u001b[39m'\u001b[39m: loss, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_test\u001b[39m\u001b[39m'\u001b[39m: loss_t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mepsilons\u001b[39m\u001b[39m'\u001b[39m: eps\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/coding/bachelorarbeit/ba_code/training/utils/trainer.py:190\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_gan()\n\u001b[1;32m    189\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdpnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     loss, acc, loss_t, acc_t, eps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dp()\n\u001b[1;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, acc, loss_t, acc_t, eps\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/coding/bachelorarbeit/ba_code/training/utils/trainer.py:244\u001b[0m, in \u001b[0;36mTrainer.train_dp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m loss \u001b[39m=\u001b[39m cross_loss\n\u001b[1;32m    243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 244\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    247\u001b[0m out_iden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(out_prob, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:69\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py:337\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     grad_sampler_fn \u001b[39m=\u001b[39m ft_compute_per_sample_gradient\n\u001b[0;32m--> 337\u001b[0m grad_samples \u001b[39m=\u001b[39m grad_sampler_fn(module, activations, backprops)\n\u001b[1;32m    338\u001b[0m \u001b[39mfor\u001b[39;00m param, gs \u001b[39min\u001b[39;00m grad_samples\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    339\u001b[0m     create_or_accumulate_grad_sample(\n\u001b[1;32m    340\u001b[0m         param\u001b[39m=\u001b[39mparam, grad_sample\u001b[39m=\u001b[39mgs, max_batch_len\u001b[39m=\u001b[39mmodule\u001b[39m.\u001b[39mmax_batch_len\n\u001b[1;32m    341\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/opacus/grad_sample/conv.py:103\u001b[0m, in \u001b[0;36mcompute_conv_grad_sample\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m# rearrange the above tensor and extract diagonals.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m grad_sample \u001b[39m=\u001b[39m grad_sample\u001b[39m.\u001b[39mview(\n\u001b[1;32m     96\u001b[0m     n,\n\u001b[1;32m     97\u001b[0m     layer\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     np\u001b[39m.\u001b[39mprod(layer\u001b[39m.\u001b[39mkernel_size),\n\u001b[1;32m    102\u001b[0m )\n\u001b[0;32m--> 103\u001b[0m grad_sample \u001b[39m=\u001b[39m contract(\u001b[39m\"\u001b[39;49m\u001b[39mngrg...->ngr...\u001b[39;49m\u001b[39m\"\u001b[39;49m, grad_sample)\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m    104\u001b[0m shape \u001b[39m=\u001b[39m [n] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    105\u001b[0m ret[layer\u001b[39m.\u001b[39mweight] \u001b[39m=\u001b[39m grad_sample\u001b[39m.\u001b[39mview(shape)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 7.78 GiB of which 90.25 MiB is free. Process 5120 has 1.95 GiB memory in use. Process 7102 has 2.18 GiB memory in use. Including non-PyTorch memory, this process has 2.59 GiB memory in use. Of the allocated memory 1.15 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(dataset=\"mnist\", mode=\"dpnn\")\n",
    "trainer.correct_module()\n",
    "loss, acc, loss_t, acc_t, eps = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/dp_training_data_celeba.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss, \n",
    "        'loss_test': loss_t,\n",
    "        'acc_train': acc,\n",
    "        'acc_test': acc_t,\n",
    "        'epsilons': eps\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'mnist', 'model_name': 'VGG16', 'gray': True, 'torch': True, 'num_classes': 10, 'local': False}\n",
      "----------------Loading datasets-----------------\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets/mnist\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=3)\n",
      "               ToTensor()\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw Graph in Tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1065: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Process started!\n",
      "Epoch:0\tTime:88.51\tTrain Loss:0.09\tTrain Acc:97.22\tTest Acc98.69\n",
      "Epoch:1\tTime:79.08\tTrain Loss:0.03\tTrain Acc:99.18\tTest Acc99.35\n",
      "Epoch:2\tTime:78.53\tTrain Loss:0.01\tTrain Acc:99.58\tTest Acc99.27\n",
      "Epoch:3\tTime:79.13\tTrain Loss:0.01\tTrain Acc:99.80\tTest Acc99.37\n",
      "Epoch:4\tTime:79.04\tTrain Loss:0.00\tTrain Acc:99.91\tTest Acc99.50\n",
      "Epoch:5\tTime:79.05\tTrain Loss:0.00\tTrain Acc:99.95\tTest Acc99.54\n",
      "Epoch:6\tTime:78.84\tTrain Loss:0.00\tTrain Acc:99.97\tTest Acc99.49\n",
      "Epoch:7\tTime:78.68\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.54\n",
      "Epoch:8\tTime:78.98\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.54\n",
      "Epoch:9\tTime:78.91\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.54\n",
      "Epoch:10\tTime:79.34\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n",
      "Epoch:11\tTime:79.09\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n",
      "Epoch:12\tTime:79.31\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.58\n",
      "Epoch:13\tTime:74.47\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.55\n",
      "Epoch:14\tTime:70.74\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_nn, acc_nn, loss_t_nn, acc_t_nn \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining_metrics/nn_training_data.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_to_save \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_train\u001b[39m\u001b[39m'\u001b[39m: loss_nn, \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_test\u001b[39m\u001b[39m'\u001b[39m: loss_t,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39macc_train\u001b[39m\u001b[39m'\u001b[39m: acc,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39macc_test\u001b[39m\u001b[39m'\u001b[39m: acc_t\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(data_to_save, file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_t' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset=\"mnist\", mode=\"nn\")\n",
    "loss_nn, acc_nn, loss_t_nn, acc_t_nn = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/nn_training_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss_nn, \n",
    "        'loss_test': loss_t_nn,\n",
    "        'acc_train': acc_nn,\n",
    "        'acc_test': acc_t_nn\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
