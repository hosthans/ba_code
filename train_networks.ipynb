{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 10:29:41.883235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from training.utils.trainer import Trainer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'mnist', 'model_name': 'VGG16', 'gray': True, 'torch': True, 'num_classes': 10, 'local': False}\n",
      "----------------Loading datasets-----------------\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw Graph in Tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1065: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Training-Process started! -> 15 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: overflow encountered in scalar divide\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "/home/bot/.local/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: overflow encountered in divide\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "12/06/2023 11:26:40:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1324: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tTime:261.28\tTrain Loss:0.81\tTrain Acc:78.81\tTest Acc91.76\n",
      "Epoch:1\tTime:231.79\tTrain Loss:0.21\tTrain Acc:93.41\tTest Acc94.85\n",
      "Epoch:2\tTime:241.53\tTrain Loss:0.15\tTrain Acc:95.15\tTest Acc95.79\n",
      "Epoch:3\tTime:251.12\tTrain Loss:0.14\tTrain Acc:95.83\tTest Acc96.30\n",
      "Epoch:4\tTime:243.15\tTrain Loss:0.12\tTrain Acc:96.23\tTest Acc96.67\n",
      "Epoch:5\tTime:243.36\tTrain Loss:0.12\tTrain Acc:96.51\tTest Acc96.90\n",
      "Epoch:6\tTime:243.37\tTrain Loss:0.11\tTrain Acc:96.69\tTest Acc96.90\n",
      "Epoch:7\tTime:242.54\tTrain Loss:0.10\tTrain Acc:96.89\tTest Acc97.06\n",
      "Epoch:8\tTime:245.54\tTrain Loss:0.10\tTrain Acc:96.97\tTest Acc97.22\n",
      "Epoch:9\tTime:246.96\tTrain Loss:0.10\tTrain Acc:97.04\tTest Acc97.39\n",
      "Epoch:10\tTime:240.03\tTrain Loss:0.09\tTrain Acc:97.36\tTest Acc97.60\n",
      "Epoch:11\tTime:236.74\tTrain Loss:0.09\tTrain Acc:97.21\tTest Acc97.55\n",
      "Epoch:12\tTime:235.94\tTrain Loss:0.09\tTrain Acc:97.30\tTest Acc97.57\n",
      "Epoch:13\tTime:240.98\tTrain Loss:0.09\tTrain Acc:97.32\tTest Acc97.78\n",
      "Epoch:14\tTime:235.40\tTrain Loss:0.09\tTrain Acc:97.47\tTest Acc97.79\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset=\"mnist\", mode=\"dpnn\")\n",
    "trainer.correct_module()\n",
    "loss, acc, loss_t, acc_t, eps = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/dp_training_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss, \n",
    "        'loss_test': loss_t,\n",
    "        'acc_train': acc,\n",
    "        'acc_test': acc_t,\n",
    "        'epsilons': eps\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'mnist', 'model_name': 'VGG16', 'gray': True, 'torch': True, 'num_classes': 10, 'local': False}\n",
      "----------------Loading datasets-----------------\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets/mnist\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=3)\n",
      "               ToTensor()\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "---------------Loading dataloader----------------\n",
      "DataLoader initialized\n",
      "DataLoader initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw Graph in Tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bot/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/bot/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1065: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
      "  module._c._create_method_from_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Process started!\n",
      "Epoch:0\tTime:88.51\tTrain Loss:0.09\tTrain Acc:97.22\tTest Acc98.69\n",
      "Epoch:1\tTime:79.08\tTrain Loss:0.03\tTrain Acc:99.18\tTest Acc99.35\n",
      "Epoch:2\tTime:78.53\tTrain Loss:0.01\tTrain Acc:99.58\tTest Acc99.27\n",
      "Epoch:3\tTime:79.13\tTrain Loss:0.01\tTrain Acc:99.80\tTest Acc99.37\n",
      "Epoch:4\tTime:79.04\tTrain Loss:0.00\tTrain Acc:99.91\tTest Acc99.50\n",
      "Epoch:5\tTime:79.05\tTrain Loss:0.00\tTrain Acc:99.95\tTest Acc99.54\n",
      "Epoch:6\tTime:78.84\tTrain Loss:0.00\tTrain Acc:99.97\tTest Acc99.49\n",
      "Epoch:7\tTime:78.68\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.54\n",
      "Epoch:8\tTime:78.98\tTrain Loss:0.00\tTrain Acc:99.99\tTest Acc99.54\n",
      "Epoch:9\tTime:78.91\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.54\n",
      "Epoch:10\tTime:79.34\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n",
      "Epoch:11\tTime:79.09\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n",
      "Epoch:12\tTime:79.31\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.58\n",
      "Epoch:13\tTime:74.47\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.55\n",
      "Epoch:14\tTime:70.74\tTrain Loss:0.00\tTrain Acc:100.00\tTest Acc99.56\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_nn, acc_nn, loss_t_nn, acc_t_nn \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining_metrics/nn_training_data.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_to_save \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_train\u001b[39m\u001b[39m'\u001b[39m: loss_nn, \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss_test\u001b[39m\u001b[39m'\u001b[39m: loss_t,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39macc_train\u001b[39m\u001b[39m'\u001b[39m: acc,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39macc_test\u001b[39m\u001b[39m'\u001b[39m: acc_t\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bot/coding/bachelorarbeit/ba_code/train_networks.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(data_to_save, file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_t' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset=\"mnist\", mode=\"nn\")\n",
    "loss_nn, acc_nn, loss_t_nn, acc_t_nn = trainer.train()\n",
    "\n",
    "with open(\"training_metrics/nn_training_data.pkl\", 'wb') as file:\n",
    "    data_to_save = {\n",
    "        'loss_train': loss_nn, \n",
    "        'loss_test': loss_t_nn,\n",
    "        'acc_train': acc_nn,\n",
    "        'acc_test': acc_t_nn\n",
    "    }\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
